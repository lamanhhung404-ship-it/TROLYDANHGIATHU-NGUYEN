import streamlit as st
import google.generativeai as genai
from PIL import Image
import tempfile
import io
import os
import pandas as pd
from docx import Document
import random
import re

# =========================================================
# 1) C·∫§U H√åNH TRANG
# =========================================================
st.set_page_config(
    page_title="Tr·ª£ l√Ω ƒë√°nh gi√° th∆∞·ªùng xuy√™n h·∫±ng th√°ng (TT27)",
    page_icon="üìù",
    layout="centered"
)

# =========================================================
# 2) CSS GIAO DI·ªÜN
# =========================================================
st.markdown("""
<style>
    [data-testid="stAppViewContainer"] { background-color: #f4f6f9; }
    .header-box {
        background: linear-gradient(135deg, #06b6d4 0%, #3b82f6 100%);
        padding: 26px; border-radius: 16px; text-align: center; color: white;
        margin-bottom: 18px; box-shadow: 0 6px 18px rgba(0,0,0,0.08);
    }
    .header-box h1 { color: white !important; margin: 0; font-size: 1.65rem; }
    .header-box p { margin: 6px 0 0 0; opacity: .95; }

    div.stButton > button {
        background: linear-gradient(90deg, #06b6d4, #3b82f6);
        color: white !important; border: none; padding: 14px;
        font-weight: 700; border-radius: 12px; width: 100%;
        font-size: 17px;
    }
    .note {
        background: #ffffff;
        border: 1px solid #e5e7eb;
        padding: 12px 14px;
        border-radius: 12px;
        margin: 10px 0;
        color: #111827;
    }
    .muted { color: #6b7280; }
</style>
""", unsafe_allow_html=True)

# =========================================================
# 3) H√ÄM TI·ªÜN √çCH
# =========================================================
FORBIDDEN_WORDS = [" em ", " con ", " b·∫°n ", " h·ªçc sinh ", "hs ", " ch√°u ", " b√© "]

def normalize_spaces(s: str) -> str:
    return re.sub(r"\s+", " ", str(s)).strip()

def clean_comment_format(text: str) -> str:
    """Chu·∫©n h√≥a: b·ªè k√Ω t·ª± ƒë·∫ßu d√≤ng, ch·ªâ vi·∫øt hoa ch·ªØ c√°i ƒë·∫ßu c√¢u."""
    if not text:
        return ""
    t = str(text).strip().strip("-*‚Ä¢").strip()
    t = normalize_spaces(t)
    if not t:
        return ""
    return t[0].upper() + t[1:]

def contains_forbidden_words(text: str) -> bool:
    t = " " + normalize_spaces(text).lower() + " "
    for w in FORBIDDEN_WORDS:
        if w in t:
            return True
    return False

def classify_monthly_level(value) -> str:
    """
    Ph√¢n lo·∫°i cho ƒê√ÅNH GI√Å TH∆Ø·ªúNG XUY√äN:
    - Ho√†n th√†nh t·ªët
    - Ho√†n th√†nh
    - C·∫ßn c·ªë g·∫Øng

    Nh·∫≠n c√°c d·∫°ng ph·ªï bi·∫øn:
    - 'T', 'HTT', 'Ho√†n th√†nh t·ªët', 'T·ªët'
    - 'H', 'HT', 'Ho√†n th√†nh', 'ƒê·∫°t'
    - 'C', 'CCG', 'C·∫ßn c·ªë g·∫Øng', 'Ch∆∞a ƒë·∫°t', 'Ch∆∞a ho√†n th√†nh'
    - C√≥ th·ªÉ nh·∫≠p s·ªë: >=8 -> HTT; >=5 -> HT; <5 -> CCG
    """
    s = str(value).strip().lower()

    if s in ["t", "htt", "ho√†n th√†nh t·ªët", "hoan thanh tot", "t·ªët", "tot"]:
        return "Ho√†n th√†nh t·ªët"
    if s in ["h", "ht", "ho√†n th√†nh", "hoan thanh", "ƒë·∫°t", "dat"]:
        return "Ho√†n th√†nh"
    if s in ["c", "ccg", "c·∫ßn c·ªë g·∫Øng", "can co gang", "ch∆∞a ƒë·∫°t", "chua dat", "ch∆∞a ho√†n th√†nh", "chua hoan thanh"]:
        return "C·∫ßn c·ªë g·∫Øng"

    # ƒëi·ªÉm s·ªë (n·∫øu c√≥)
    try:
        score = float(s.replace(",", "."))
        if score >= 8:
            return "Ho√†n th√†nh t·ªët"
        if score >= 5:
            return "Ho√†n th√†nh"
        return "C·∫ßn c·ªë g·∫Øng"
    except:
        return "Ho√†n th√†nh"

def extract_context_from_evidence(evidence_files):
    """
    Tr√≠ch 'Minh ch·ª©ng' t·ª´ docx (text), pdf/image (media) ƒë·ªÉ ƒë∆∞a v√†o Gemini.
    Tr·∫£ v·ªÅ: (context_text, media_files)
    """
    context_text = ""
    media_files = []

    if not evidence_files:
        return context_text, media_files

    for file in evidence_files:
        try:
            name = getattr(file, "name", "").lower()

            # DOCX -> l·∫•y text
            if name.endswith(".docx"):
                doc = Document(file)
                context_text += "\n".join([p.text for p in doc.paragraphs if p.text.strip()])
                context_text += "\n"

            # PDF -> upload file cho Gemini
            elif getattr(file, "type", "") == "application/pdf" or name.endswith(".pdf"):
                with tempfile.NamedTemporaryFile(delete=False, suffix=".pdf") as tmp:
                    tmp.write(file.getvalue())
                    tmp.flush()
                    media_files.append(genai.upload_file(tmp.name))
                try:
                    os.unlink(tmp.name)
                except:
                    pass

            # ·∫¢nh -> m·ªü ·∫£nh
            else:
                img = Image.open(file)
                media_files.append(img)

        except:
            # b·ªè qua file l·ªói
            pass

    return context_text, media_files

def parse_ai_bullets_by_section(content: str) -> dict:
    """
    AI tr·∫£ v·ªÅ theo c·∫•u tr√∫c:
    I. M·ª®C: HO√ÄN TH√ÄNH T·ªêT
    - ...
    II. M·ª®C: HO√ÄN TH√ÄNH
    - ...
    III. M·ª®C: C·∫¶N C·ªê G·∫ÆNG
    - ...

    Tr·∫£ v·ªÅ:
    {"Ho√†n th√†nh t·ªët":[...], "Ho√†n th√†nh":[...], "C·∫ßn c·ªë g·∫Øng":[...]}
    """
    sections = {"Ho√†n th√†nh t·ªët": [], "Ho√†n th√†nh": [], "C·∫ßn c·ªë g·∫Øng": []}
    current = None

    lines = str(content).splitlines()
    for raw in lines:
        line = raw.strip()
        if not line:
            continue

        up = line.upper()

        if "M·ª®C" in up and "HO√ÄN TH√ÄNH T·ªêT" in up:
            current = "Ho√†n th√†nh t·ªët"
            continue
        if "M·ª®C" in up and ("C·∫¶N C·ªê G·∫ÆNG" in up or "CAN CO GANG" in up):
            current = "C·∫ßn c·ªë g·∫Øng"
            continue
        if "M·ª®C" in up and "HO√ÄN TH√ÄNH" in up:
            current = "Ho√†n th√†nh"
            continue

        is_bullet = (
            line.startswith("-")
            or line.startswith("*")
            or re.match(r"^\d+[\.\)]\s+", line)
        )
        if current and is_bullet:
            text = re.sub(r"^[-*]\s*", "", line)
            text = re.sub(r"^\d+[\.\)]\s*", "", text)
            text = text.replace("**", "").strip()
            text = clean_comment_format(text)

            # l·ªçc c√¢u qu√° ng·∫Øn + l·ªçc t·ª´ c·∫•m
            if len(text) >= 35 and not contains_forbidden_words(text):
                sections[current].append(text)

    return sections

def ensure_unique_pool(pool: list, needed: int, fallback: str) -> list:
    """
    L·ªçc tr√πng + ƒë·∫£m b·∫£o ƒë·ªß s·ªë l∆∞·ª£ng.
    N·∫øu thi·∫øu, b√π b·∫±ng c√¢u fallback ƒë·ªÉ tr√°nh l·ªói.
    """
    seen = set()
    uniq = []
    for s in pool:
        key = normalize_spaces(s).lower()
        if key not in seen:
            seen.add(key)
            uniq.append(s)

    while len(uniq) < needed:
        uniq.append(fallback)

    random.shuffle(uniq)
    return uniq

# =========================================================
# 4) GIAO DI·ªÜN
# =========================================================
st.markdown("""
<div class="header-box">
  <h1>üìù TR·ª¢ L√ù ƒê√ÅNH GI√Å TH∆Ø·ªúNG XUY√äN H·∫∞NG TH√ÅNG (TT27)</h1>
  <p>Nh·∫≠p danh s√°ch ‚Üí t·∫°o nh·∫≠n x√©t c√° nh√¢n h√≥a theo m·ª©c ƒë·ªô ‚Üí xu·∫•t Excel</p>
  <p><b>T√°c gi·∫£:</b> La M·∫°nh H√πng ‚Äì Tr∆∞·ªùng PTDTBT TH&THCS N√† Kh∆∞∆°ng</p>
</div>
""", unsafe_allow_html=True)

st.markdown("""
<div class="note">
  <div><b>L∆∞u √Ω:</b> ƒê√°nh gi√° th∆∞·ªùng xuy√™n t·∫≠p trung m√¥ t·∫£ bi·ªÉu hi·ªán, m·ª©c ƒë·ªô ho√†n th√†nh v√† ƒë·ªãnh h∆∞·ªõng r√®n luy·ªán; kh√¥ng ch·∫•m ƒëi·ªÉm, kh√¥ng so s√°nh.</div>
  <div class="muted">Quy t·∫Øc m·∫∑c ƒë·ªãnh: kh√¥ng d√πng ‚Äúem / con / b·∫°n / h·ªçc sinh / HS‚Äù.</div>
</div>
""", unsafe_allow_html=True)

# =========================================================
# 5) API KEY
# =========================================================
with st.sidebar:
    st.header("üîê C·∫•u h√¨nh AI")
    default_key = st.secrets["GEMINI_API_KEY"] if "GEMINI_API_KEY" in st.secrets else ""
    manual_key = st.text_input("üîë Nh·∫≠p Gemini API Key:", type="password")

    if manual_key:
        api_key = manual_key
        st.info("ƒêang d√πng key c√° nh√¢n")
    elif default_key:
        api_key = default_key
        st.success("ƒêang d√πng key h·ªá th·ªëng (secrets)")
    else:
        api_key = None
        st.warning("Ch∆∞a c√≥ API key!")

    st.markdown("---")
    st.header("‚öôÔ∏è T√πy ch·ªçn")
    model_name = st.selectbox(
        "Model Gemini:",
        ["gemini-2.5-flash", "gemini-2.5-flash-lite-preview-09-2025", "gemini-2.0-flash"],
        index=1
    )
    st.caption("N·∫øu ch·∫°y ch·∫≠m, ch·ªçn flash-lite ƒë·ªÉ nhanh h∆°n.")

if api_key:
    try:
        genai.configure(api_key=api_key)
    except Exception:
        st.error("API key c√≥ v·∫ª kh√¥ng h·ª£p l·ªá. Vui l√≤ng ki·ªÉm tra l·∫°i.")

# =========================================================
# 6) INPUT
# =========================================================
st.info("B∆∞·ªõc 1: T·∫£i file danh s√°ch h·ªçc sinh v√† (n·∫øu c√≥) minh ch·ª©ng.")
c1, c2 = st.columns(2)
with c1:
    student_file = st.file_uploader("üìÇ Danh s√°ch HS (.xlsx/.xls):", type=["xlsx", "xls"])
with c2:
    evidence_files = st.file_uploader(
        "üìé Minh ch·ª©ng (·∫£nh/docx/pdf) - t√πy ch·ªçn:",
        type=["pdf", "png", "jpg", "jpeg", "docx"],
        accept_multiple_files=True
    )

# =========================================================
# 7) X·ª¨ L√ù FILE HS
# =========================================================
if student_file:
    df = pd.read_excel(student_file)
    st.write("‚ñº Xem nhanh 5 d√≤ng ƒë·∫ßu:")
    st.dataframe(df.head(5), use_container_width=True)
    st.markdown("---")

    col_level = st.selectbox("üìå Ch·ªçn c·ªôt M·ª®C/ƒêI·ªÇM/ƒê·∫†T:", df.columns)
    col_new = st.text_input("üìå T√™n c·ªôt nh·∫≠n x√©t m·ªõi:", "Nh·∫≠n x√©t ƒêGTX (th√°ng)")

    c3, c4 = st.columns(2)
    with c3:
        mon_hoc = st.text_input("üìö M√¥n:", "To√°n")
    with c4:
        thang = st.selectbox(
            "üóìÔ∏è Th√°ng ƒë√°nh gi√°:",
            ["Th√°ng 1", "Th√°ng 2", "Th√°ng 3", "Th√°ng 4", "Th√°ng 5", "Th√°ng 6",
             "Th√°ng 7", "Th√°ng 8", "Th√°ng 9", "Th√°ng 10", "Th√°ng 11", "Th√°ng 12"],
            index=0
        )

    chu_de = st.text_input("üìù N·ªôi dung/ch·ªß ƒë·ªÅ/tu·∫ßn (ghi r√µ):", "Ch·ªß ƒë·ªÅ/B√†i h·ªçc trong th√°ng")
    tieu_chi = st.text_area(
        "‚úÖ Ti√™u ch√≠ quan s√°t (t√πy ch·ªçn - c√†ng c·ª• th·ªÉ c√†ng t·ªët):",
        value="Tham gia h·ªçc t·∫≠p; th·ª±c hi·ªán nhi·ªám v·ª•; h·ª£p t√°c; v·∫≠n d·ª•ng ki·∫øn th·ª©c; tu√¢n th·ªß quy ƒë·ªãnh."
    )

    st.markdown("---")
    st.subheader("B∆∞·ªõc 2: T·∫°o nh·∫≠n x√©t")
    st.caption("H·ªá th·ªëng s·∫Ω ƒë·∫øm s·ªë l∆∞·ª£ng theo 3 m·ª©c: Ho√†n th√†nh t·ªët / Ho√†n th√†nh / C·∫ßn c·ªë g·∫Øng.")

    if st.button("üöÄ T·∫†O NH·∫¨N X√âT ƒê√ÅNH GI√Å TH∆Ø·ªúNG XUY√äN (KH√îNG TR√ôNG L·∫∂P)"):
        if not api_key:
            st.error("Thi·∫øu API key Gemini. Vui l√≤ng nh·∫≠p trong thanh b√™n.")
            st.stop()

        progress = st.progress(0, text="ƒêang ph√¢n lo·∫°i h·ªçc sinh theo m·ª©c...")

        df["__Level__"] = df[col_level].apply(classify_monthly_level)
        counts = df["__Level__"].value_counts()

        count_T = int(counts.get("Ho√†n th√†nh t·ªët", 0))
        count_H = int(counts.get("Ho√†n th√†nh", 0))
        count_C = int(counts.get("C·∫ßn c·ªë g·∫Øng", 0))

        st.write(f"üìä S·ªë l∆∞·ª£ng: **HTT={count_T}**, **HT={count_H}**, **CCG={count_C}**")

        progress.progress(15, text="ƒêang x·ª≠ l√Ω minh ch·ª©ng (n·∫øu c√≥)...")
        context_text, media_files = extract_context_from_evidence(evidence_files)

        req_T = int(count_T * 1.1) + 2
        req_H = int(count_H * 1.1) + 2
        req_C = int(count_C * 1.1) + 2

        progress.progress(30, text="ƒêang g·ªçi AI t·∫°o nh·∫≠n x√©t...")

        model = genai.GenerativeModel(model_name)

        prompt = f"""
B·∫°n l√† gi√°o vi√™n ti·ªÉu h·ªçc. Vi·∫øt nh·∫≠n x√©t ƒê√ÅNH GI√Å TH∆Ø·ªúNG XUY√äN H·∫∞NG TH√ÅNG theo tinh th·∫ßn TT27.

TH√îNG TIN:
- Th√°ng: {thang}
- M√¥n: {mon_hoc}
- N·ªôi dung/ch·ªß ƒë·ªÅ trong th√°ng: {chu_de}
- Ti√™u ch√≠ quan s√°t: {tieu_chi}
- Minh ch·ª©ng t√≥m t·∫Øt (n·∫øu c√≥): {context_text[:2000]}

QUY T·∫ÆC B·∫ÆT BU·ªòC:
1) KH√îNG d√πng c√°c t·ª´: "em", "con", "b·∫°n", "h·ªçc sinh", "HS", "ch√°u", "b√©".
2) KH√îNG ch·∫•m ƒëi·ªÉm, KH√îNG ghi s·ªë ƒëi·ªÉm, KH√îNG x·∫øp lo·∫°i/so s√°nh.
3) VƒÉn phong s∆∞ ph·∫°m, n√™u bi·ªÉu hi·ªán c·ª• th·ªÉ, ng·∫Øn g·ªçn: 120‚Äì180 k√Ω t·ª±/c√¢u.
4) M·ªói c√¢u ƒë·ªß √Ω, tr√°nh chung chung; KH√îNG vi·∫øt in hoa to√†n b·ªô.
5) M·ªói c√¢u KH√ÅC NHAU (kh√¥ng tr√πng l·∫∑p).

S·ªê L∆Ø·ª¢NG (B·∫ÆT BU·ªòC ƒê·ª¶):
- {req_T} c√¢u cho m·ª©c: HO√ÄN TH√ÄNH T·ªêT
- {req_H} c√¢u cho m·ª©c: HO√ÄN TH√ÄNH
- {req_C} c√¢u cho m·ª©c: C·∫¶N C·ªê G·∫ÆNG

G·ª¢I √ù N·ªòI DUNG THEO M·ª®C:
A) HO√ÄN TH√ÄNH T·ªêT: ghi nh·∫≠n r√µ k·ªπ nƒÉng/ti·∫øn b·ªô + th√°i ƒë·ªô h·ªçc t·∫≠p t√≠ch c·ª±c; KH√îNG d√πng "nh∆∞ng/tuy nhi√™n".
B) HO√ÄN TH√ÄNH: n√™u ƒëi·ªÉm l√†m ƒë∆∞·ª£c + "tuy nhi√™n/nh∆∞ng" + 1 ƒëi·ªÉm c·∫ßn r√®n trong th√°ng t·ªõi (c·ª• th·ªÉ).
C) C·∫¶N C·ªê G·∫ÆNG: ghi nh·∫≠n s·ª± tham gia/ƒëi·ªÉm m·∫°nh nh·ªè + "nh∆∞ng" + c·∫ßn h·ªó tr·ª£/r√®n g√¨ (c·ª• th·ªÉ, nh·∫π nh√†ng).

ƒê·ªäNH D·∫†NG TR·∫¢ V·ªÄ (CH√çNH X√ÅC):
I. M·ª®C: HO√ÄN TH√ÄNH T·ªêT
- ...
II. M·ª®C: HO√ÄN TH√ÄNH
- ...
III. M·ª®C: C·∫¶N C·ªê G·∫ÆNG
- ...
"""

        inputs = [prompt] + media_files

        try:
            response = model.generate_content(inputs)
            ai_text = response.text if hasattr(response, "text") else str(response)

            progress.progress(60, text="ƒêang t√°ch c√¢u v√† ph√¢n ph·ªëi cho t·ª´ng h·ªçc sinh...")

            sections = parse_ai_bullets_by_section(ai_text)

            pool_T = ensure_unique_pool(
                sections.get("Ho√†n th√†nh t·ªët", []),
                count_T,
                fallback="Ho√†n th√†nh t·ªët nhi·ªám v·ª• trong th√°ng, ch·ªß ƒë·ªông th·ª±c hi·ªán b√†i t·∫≠p v√† th·ªÉ hi·ªán th√°i ƒë·ªô h·ªçc t·∫≠p t√≠ch c·ª±c."
            )
            pool_H = ensure_unique_pool(
                sections.get("Ho√†n th√†nh", []),
                count_H,
                fallback="Ho√†n th√†nh nhi·ªám v·ª• h·ªçc t·∫≠p trong th√°ng, tuy nhi√™n c·∫ßn r√®n th√™m t√≠nh c·∫©n th·∫≠n v√† duy tr√¨ t·∫≠p trung khi th·ª±c hi·ªán b√†i."
            )
            pool_C = ensure_unique_pool(
                sections.get("C·∫ßn c·ªë g·∫Øng", []),
                count_C,
                fallback="C√≥ tham gia ho·∫°t ƒë·ªông h·ªçc t·∫≠p, nh∆∞ng c·∫ßn ƒë∆∞·ª£c h∆∞·ªõng d·∫´n th√™m ƒë·ªÉ ho√†n th√†nh nhi·ªám v·ª• ƒë√∫ng y√™u c·∫ßu v√† duy tr√¨ th√≥i quen t·ª± luy·ªán t·∫≠p."
            )

            def assign_comment(level: str) -> str:
                if level == "Ho√†n th√†nh t·ªët":
                    return pool_T.pop(0) if pool_T else "Ho√†n th√†nh t·ªët nhi·ªám v·ª• trong th√°ng."
                if level == "Ho√†n th√†nh":
                    return pool_H.pop(0) if pool_H else "Ho√†n th√†nh nhi·ªám v·ª• trong th√°ng, c·∫ßn r√®n th√™m m·ªôt s·ªë n·ªôi dung."
                if level == "C·∫ßn c·ªë g·∫Øng":
                    return pool_C.pop(0) if pool_C else "C·∫ßn c·ªë g·∫Øng h∆°n trong th√°ng t·ªõi, c·∫ßn h·ªó tr·ª£ ƒë·ªÉ ho√†n th√†nh nhi·ªám v·ª•."
                return "Ho√†n th√†nh nhi·ªám v·ª• trong th√°ng."

            df[col_new] = df["__Level__"].apply(assign_comment)
            df.drop(columns=["__Level__"], inplace=True)

            progress.progress(85, text="ƒêang xu·∫•t Excel k·∫øt qu·∫£...")

            output = io.BytesIO()
            with pd.ExcelWriter(output, engine="openpyxl") as writer:
                df.to_excel(writer, index=False, sheet_name="Danh s√°ch")
                ws = writer.sheets["Danh s√°ch"]
                try:
                    from openpyxl.utils import get_column_letter
                    col_idx = df.columns.get_loc(col_new) + 1
                    ws.column_dimensions[get_column_letter(col_idx)].width = 65
                except:
                    pass

            output.seek(0)

            progress.progress(100, text="Ho√†n t·∫•t!")

            st.success("‚úÖ ƒê√£ t·∫°o nh·∫≠n x√©t ƒë√°nh gi√° th∆∞·ªùng xuy√™n h·∫±ng th√°ng (m·ªói h·ªçc sinh 1 c√¢u).")
            st.download_button(
                "‚¨áÔ∏è T·∫£i file Excel k·∫øt qu·∫£",
                data=output,
                file_name=f"NhanXet_DGTX_{thang.replace(' ', '')}_{mon_hoc}.xlsx",
                mime="application/vnd.openxmlformats-officedocument.spreadsheetml.sheet"
            )

            with st.expander("üîé Ki·ªÉm tra ng·∫´u nhi√™n 8 h·ªçc sinh"):
                cols_show = [c for c in [col_level, col_new] if c in df.columns]
                st.dataframe(df.sample(min(8, len(df)))[cols_show], use_container_width=True)

            with st.expander("üßæ Xem n·ªôi dung AI ƒë√£ tr·∫£ v·ªÅ (ƒë·ªÉ ki·ªÉm tra)"):
                st.text(ai_text[:8000])

        except Exception as e:
            st.error(f"L·ªói x·ª≠ l√Ω: {e}")

# =========================================================
# 8) FOOTER
# =========================================================
st.markdown("""
<hr>
<div style="text-align:center; color:#6b7280; font-size:14px;">
¬© 2026 ‚Äì La M·∫°nh H√πng ‚Äì Tr∆∞·ªùng PTDTBT TH&THCS N√† Kh∆∞∆°ng<br>
Tr·ª£ l√Ω ƒë√°nh gi√° th∆∞·ªùng xuy√™n h·∫±ng th√°ng h·ªçc sinh ti·ªÉu h·ªçc theo TT27
</div>
""", unsafe_allow_html=True)
